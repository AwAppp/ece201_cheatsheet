\documentclass{report}

\title{ECE 205A midterm cheat sheet}
\author{AnPing Tao}
\date{\today}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{color}
\usepackage{cancel}
\usepackage{pdfpages}
\usepackage{multirow}
\usepackage{rotating}

\def\changemargin{\list{}{\rightmargin 0.5cm \leftmargin 0.5cm}\item[]}
\let\endchangemargin=\endlist

\renewcommand{\qedsymbol}{$\blacksquare$}

\setlength{\parindent}{0pt}

% \definecolor{mybg}{RGB}{72, 38, 82}
% \definecolor{mytext}{RGB}{177, 215, 183}

% \color{mytext}
% \pagecolor{mybg}

\begin{document}

\textbf{Def:} A \underline{vector} is a tuple of numbers.

\textbf{Def:} The \underline{dimension} is the size of this tuple.

\textbf{Def:} A \underline{matrix} is a 2-dimensional grid of numbers.

\textbf{Def:} $\mathbb{R}^{m \times n}$ denotes all $m \times n$ matrices with field $\mathbb{R}$.

\textbf{Def:} $\mathbb{C}^{m \times n}$ denotes all $m \times n$ matrices with field $\mathbb{C}$.

\textbf{Def:} vector-matrix product:
$\begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix} \cdot
    \begin{bmatrix}
        x \\y
    \end{bmatrix} =
    \begin{bmatrix}
        ax + by \\
        cx + dy
    \end{bmatrix}$
(Inner dimensions must agree. Outer dimensions remain.)

$\begin{bmatrix}
        ax + by \\
        cx + dy
    \end{bmatrix} =
    x \begin{bmatrix}
        a \\c
    \end{bmatrix} + y
    \begin{bmatrix}
        b \\d
    \end{bmatrix}$: Express matrix-vector multiplication as a linear combination of the matrix.

\textbf{Def:} \underline{Inner product}
$<\begin{bmatrix}
        x_1 \\ x_2 \\ \vdots \\ x_n
    \end{bmatrix},
    \begin{bmatrix}
        y_1 \\ y_2 \\ \vdots \\ y_n
    \end{bmatrix}> =
    \begin{bmatrix}
        x_1 & x_2 & \cdots & x_n
    \end{bmatrix}
    \begin{bmatrix}
        y_1 \\ y_2 \\ \vdots \\ y_n
    \end{bmatrix} = x_1y_1 + x_2y_2 + \cdots + x_ny_n$

\textbf{Def:} \underline{Matrix Multiplication:} $A \in \mathbb{R}^{m \times k}, B \in \mathbb{R}^{k \times n}$,
$C = AB \in \mathbb{R}^{m \times n}$ and $C_{ij} = \sum_{k=1}^{n} A_{ik}B_{kj}$.

\textbf{Def:} \underline{span} of vectors $v_1, v_2, \dots, v_n$ is the set of vectors that can be obtained as
linear combination of the vectors $v_1, v_2, \dots, v_n$:

$span\{v_1, v_2, \dots, v_n\} = a_1v_1 + a_2v_2 + \dots + a_nv_n$ for $a_1, a_2, \dots, a_n$ are scalars.

\textbf{Def:} A set of verctor $v_1, v_2, \dots, v_n$ is said to be \underline{lienarly
    independent} if none of these vectors can be expressed as a linear combination
of others.

\vspace{2mm}
\underline{Solving a system of linear equations}

\textbf{Def:} Augmented matrix - append $b$ to $A$, where $A \in
    \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^{n}$.

\textbf{Def:} A \underline{pivot} in a row is its leftmost non-zero element.

\textbf{Note:} if the column spafe of $A$, $\mathcal{C}(A)$ is LD,
there is no soln or infinite number solns.

\underline{Matrix Multiplication}: $AB = C$ iff $C_{ik} = <A_i^T, B_k>$.

Application: find the walk of length $n$ from node $i$ to $j$ in a graph.
The answer will be $A^{n}_{ij}$, where $A$ is the adjacency matrix of
the graph.

\textbf{Thm:} If $A^{-1}A = I$ and $A\tilde{A}^{-1} = I$, then $A^{-1}
    = \tilde{A}^{-1}$

\textbf{Def:} $A$ is \underline{symmetric} iff $A^{T} = A$.

\textbf{Def:} A matrix whose transpose is also its inverse is
called \underline{orthogonal}. $A^{T}A = I \implies <col\ i, col\ j> =
    \delta_{ij}$.

\textbf{Def:} \underline{length} of a vector.
$\|v\| = \sqrt{<v, v>}$. (Also known as L2 norm).

\textbf{Def:} \underline{Angle} between two vectors.
$\frac{<v, w>}{\|v\| \|w\|} = \cos \theta$.

\vspace{2mm}

\textbf{Def:} \underline{Permutation} is the operation where in each
element in the pre-image appears exactly once in the image.

Properties of a permutation matrix:
\begin{itemize}
    \item square matrix
    \item there is a one '1' per every row/col
    \item all rows are distinct, all columns are distinct.
\end{itemize}

Suppose $A$ and $B$ are both permutation matrices of the same dimension.
$AB$ is another permutation.

Permutations don't change the length of the vector and
the between vectors.

\textbf{Def:} \underline{Transpose} $A^{T}_{ij} = A_{ji}$. $A$ does NOT
need to be square.

\textbf{Thm:} If $A$ is a permutation matrix, then its
transpose is its inverse. Moreover, $A$ is also an \underline{orthogonal}
matrix.

\vspace{2mm}

\underline{Rotation}: matrix
$\begin{bmatrix}
        \cos \theta & -\sin \theta \\
        \sin \theta & \cos \theta
    \end{bmatrix}$,
one can show that apply a rotation matrix to a vector does NOT
alter its length. Rotation matrices are orthogonal.

\vspace{5mm}

\underline{Vector Space}

\textbf{Def:} A \underline{vector Space} over $\mathbb{R}$ is a set $V$
with rules
\begin{enumerate}
    \item closed under vector addition.
    \item closed under scalar multiplication.
\end{enumerate}

\textbf{Def:} If $V$ is a vector space, $S$ is a subspace of $V$ if
\begin{enumerate}
    \item $\forall v,w \in S: v + w \in S$
    \item $\forall a \in \mathbb{R}, \forall v \in S: av \in S$
    \item $\vec{0} \in S$
\end{enumerate}

\textbf{Def:} \underline{Column Space} of matrix $A$ is the set of all linear
combination of columns of $A$.

Consider $A_{n \times n}$ a square matrix, $A$ is invertible iff
$\mathcal{C}(A) = \mathbb{R}^{n}$.

\textbf{Def:} \underline{Null space} of a matrix $A$:
$\mathcal{N}(A) = \{x | Ax = 0\}$.

\textbf{Def:} \underline{Column rank} is the number of linearly independent
column.

\textbf{Def: }A collection of vectors $U = \{v_1, v_2, \dots, v_n\}$ is
said to be \underline{linearly independent} if $\sum_{i=1}^{n} a_i v_i = \vec{0}$
only has $a_1 = a_2 = \dots = a_n = 0$ as the solution. Moreover,
$span\{U\} = \{\sum_{i=1}^{n} a_i v_i\}$.

\textbf{Def:} A set of vectors $U$ is said to be a basis for a vector
space $V$ if
\begin{enumerate}
    \item $span(U) = V$.
    \item $U$ is LI.
\end{enumerate}

\textbf{Def:} Two vectors (in the same vector space) are orthogonal if
the angle between them is $90^\circ$.

\textbf{Def:} Two subspaces $V, W \subseteq \mathbb{R}^n$ are orthogonal if
$\forall v \in V \forall w \in W : v^T w = 0$.

\textbf{Def:} Given $V \subseteq \mathbb{R}^n$, its \underline{orthogonal complement}
is $V^{\perp} = \{w \in \mathbb{R}^n | w^T v = 0 \ \forall v\in V\}$.

\textbf{Def:} \underline{orthogonal decomposition} If $V$ and $W$ form a
pair of subspaces that are orthogonal complements of each other in $\mathbb{R}^n$,
then any vector $z \in \mathbb{R}^n$ can be written as $z = v + w$, where
$v \in V, w \in W$.

\textbf{Def:} \underline{projection} of $v$ onto $w$: $proj_{w}(v) =
    \frac{<v, W>}{<w, w>} w = aw$

\textbf{Def:} \underline{Gram-Shmidts procedure}
$\begin{cases}
        w_1 = v_1 \\
        w_n = v_n - \sum_{i=1}^{n-1} proj_{w_i}v_n
    \end{cases}$
and finally normalize all vector $w$.

\underline{Linear transformation and determinant}

\textbf{Def:} Suppose $V$ and $W$ are vector spaces.
$\mathcal{L}: V \rightarrow W$ is a linear transformation if
$\mathcal{L}(\alpha v + \beta w) = \alpha \mathcal{L}(v) +
    \beta \mathcal{L}(w) \ \forall \alpha, \beta \in \mathbb{R}, \forall
    v, w \in V$.

\underline{Determinant} of a square matrix - to understand how the volume of a set
$S$ changes under the transformation $\Phi$. $\det(AB) = \det(A) \cdot \det(B)$.

\textbf{Thm:} Consider matrix $A \in \mathbb{R}^{m \times n}$. $[\mathcal{N}(A)]^{\perp} = \mathcal{C}(A^{T})$.

\textbf{Thm:} Consider $V \subseteq \mathbb{R}^n$. $\dim(V) + \dim(V^{\perp}) = n$.

\textbf{Thm:} If $A \in \mathbb{R}^{m \times n}$ then $Rank(A) + nullity(A) = n$, where $nullity(A) = \dim(\mathcal{N}(A))$.

\textbf{Thm:} Suppose matrix $A$ has both a left inverse and a right inverse, then they are the same, and are unique.

\textbf{Thm:} One right inverse (if it exists) is $A^{-R} = A^T(AA^T)^{-1}$.

\textbf{Thm:} One left inverse (if it exists) is $A^{-l} = (A^TA)^{-1}A^T$.

\textbf{Thm:} Consider $A \in \mathbb{R}^{m \times n}$. $A$ has a right inverse iff it has full row rank.

\textbf{Thm:} Consider $A \in \mathbb{R}^{m \times n}$. $A$ has a left inverse iff it has full column rank.
\end{document}